\documentclass[11pt]{article}

\usepackage{listings}
\lstset{basicstyle=\ttfamily, tabsize=4, columns=flexible, breaklines=true, stepnumber=1, numberstyle=\tiny, numbersep=6pt, xleftmargin=1.8em}
\usepackage[english]{babel}
\usepackage{parskip}


\begin{document}

\author{Roy Triesschijn, Roan Kattouw and Jan Paul Posma}
\date{\today}
\title{Transparent distributed caching over REST}

\maketitle

\section*{Context}
For websites and web applications it is important that pages are served quickly. Usually a database is used to store information, and it can take a little while to retrieve this information. Then this information has to be processed and inserted in a layout, which can also take time. In order to make the serving of often-accessed information more performant, caching solutions can be used. A possibility would be to use a local cache in memory, but the amount of information that can be stored in such a cache is limited. Therefore, a good option for a caching layer instead or on top of this local cache, would be a distributed cache. Such a cache is programmed for performance and can typically only function as a key-value store.

A popular example of a distributed cache is \emph{memcached}. Memcached uses a server application which is really fast but does not have any logic functions at all. The client has to select which server it approaches for retrieving and storing a value for a certain key. This is usually done by a hashing function, but depends on the implementation of the client library and might therefore be inconsistent when using multiple implementations or programming languages sharing the same cached data.

Another problem of memcached is that it uses its own API. While there currently are implementations for most programming languages, it can also be convenient to use standard protocols such as REST over HTTP, while not having to worry about which server to select.

\section*{Goal}
This, indeed, is the main objective of our distributed caching system. To provide a convenient cache that is easy to set up and easy to access. While it should be performant as it is a caching server, this is not the main objective, as this is mostly an academic exercise. It should provide a transparent API, where any user can call any of the caching servers, and the server should then silently pass on the request to the correct server.

\section*{Design}
Each node runs a very basic HTTP server that listens for GET and POST requests. Requests using
other methods are silently ignored. The server then does some very basic parsing on the first line
of the request to obtain the requested cache key.

This cache key is then run through a hashing algorithm that determines which caching server is
supposed to have the cache entry for that key. An RMI call is then issued to this server to
get the value of the cache key if the request method was GET, or to set it to the contents
of the request body if the request method was POST.

The remote server receiving this RMI call (which may or may not be the same server that
handled the HTTP request; this is handled completely transparently) has an in-memory
key-value store. For a set call, it writes the provided key-value pair to the store,
overwriting a pre-existing pair with the same key, if present. For a get call, it
retrieves the key-value pair with the provided key from the store and returns its
value.

After the RMI call finishes, the HTTP server returns the requested value for a GET
request, or an empty response for a POST request.

\section*{Implementation}
\subsection*{HTTP request format}
The requests the HTTP server expects and the responses it sends are very simple,
and were kept minimal. What follows is a brief description of what happens on the
HTTP layer when getting and setting values.

\subsection*{GET request format}
When getting the value of a cache key, the following HTTP request-response
dialog takes place:

\begin{lstlisting}
GET /mykey HTTP/1.0

HTTP/1.0 200 OK
Date: Mon, 11 Apr, 2011 16:56:04 CEST
Content-Type: text/plain; charset=utf-8
Content-Length: 12
Connection: close

Hello World!
\end{lstlisting}

The HTTP server responds immediately after receiving the first line with the
request method (\lstinline!GET!) and the URL (\lstinline!/mykey!). Clients
compliant with the HTTP specification will send more headers after this line,
but those will be silently ignored.

The response contains the minimal headers required by HTTP (200 response code,
date, content type and content length) as well as \lstinline!Connection: close!
to prevent HTTP 1.1 clients with keepalive functionality to try to keep the
connection open after the request has finished. The response body contains
the requested value.

\subsection*{POST request format}
When setting the value of a cache key, the following HTTP request-response
dialog takes place:

\begin{lstlisting}
POST /mykey HTTP/1.0
Content-Length: 12

Hello World!


HTTP/1.0 200 OK
Date: Mon, 11 Apr, 2011 16:57:58 CEST
Content-Type: text/plain; charset=utf-8
Content-Length: 0
Connection: close
\end{lstlisting}

In this case, the HTTP server cannot respond immediately and ignore all headers,
because the \lstinline!Content-Length! header needs to be specified. All other
headers are still ignored. After the headers, the server expects an empty line,
followed by the request body, which contains the value to set. As long as the
server has received fewer bytes than the specified number, it will wait for
more data. Once it has received enough, it will truncate the received data
to the length specified in the \lstinline!Content-Length! header, store it
in the cache, and send the response.

The response is the same as for the GET request above, except that it has an
empty body and a content length of zero.

\subsection*{Local cache}
All caches --- local and remote --- implement the interface \texttt{ICache}.

The implementation of a basic functional local cache (\texttt{BasicCache}) is quite simple. We use a \texttt{HashMap} to store key-value pairs of strings. To decide which values to throw away when the cache is full, we use a \texttt{LinkedList}. The last value of this list is removed when the list is full, and new items are placed in front. This gives almost a FIFO behavior, the only difference being that when an already existing value is set again, it is not moved to the front of the list.

\end{document}
